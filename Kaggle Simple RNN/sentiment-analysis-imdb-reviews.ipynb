{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reviews.txt', 'labels.txt']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nreviews = pd.read_csv(\"../input/reviews.txt\", header=None)\\nreviews = list(reviews.iloc[:].values)\\nlabel = pd.read_csv(\"../input/labels.txt\", header=None)\\nlabels = label.iloc[:].values\\n#print(reviews[:1])\\nencoded_label = np.array([1 if label == \"positive\" else 0 for label in labels])\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Input\n",
    "'''\n",
    "reviews = pd.read_csv(\"../input/reviews.txt\", header=None)\n",
    "reviews = list(reviews.iloc[:].values)\n",
    "label = pd.read_csv(\"../input/labels.txt\", header=None)\n",
    "labels = label.iloc[:].values\n",
    "#print(reviews[:1])\n",
    "encoded_label = np.array([1 if label == \"positive\" else 0 for label in labels])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "pos_counts = Counter()\n",
    "neg_counts = Counter()\n",
    "tot_counts = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/reviews.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('../input/labels.txt', 'r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "from string import punctuation\n",
    "reviews = reviews.lower()\n",
    "corpus = ''.join([c for c in reviews if c not in punctuation])\n",
    "reviews = corpus.split(\"\\n\")\n",
    "corpus = ' '.join(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "labels = labels.split(\"\\n\")\n",
    "labels = np.array([1 if label == \"positive\" else 0 for label in labels])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different Vocabularies\n",
    "for i in range(len(reviews)):\n",
    "    if(labels[i]):\n",
    "        for word in reviews[i].split(\" \"):\n",
    "            pos_counts[word] += 1\n",
    "            tot_counts[word] += 1\n",
    "    else:\n",
    "        for word in reviews[i].split(\" \"):\n",
    "            neg_counts[word] += 1\n",
    "            tot_counts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_ints = []\n",
    "for review in reviews:\n",
    "    reviews_ints.append([tot_counts[word] for word in review.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = corpus.split()\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "reviews_ints = []\n",
    "for review in reviews:\n",
    "    reviews_ints.append([vocab_to_int[word] for word in review.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n",
    "\n",
    "# remove 0-length reviews and their labels\n",
    "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
    "labels = np.array([labels[ii] for ii in non_zero_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Return features of review_ints, where each review is padded with 0's if len less than seq_length\n",
    "or truncated to the input seq_length.\n",
    "'''\n",
    "def pad_features(reviews_ints, seq_length):\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and \n",
    "    for i, row in enumerate(reviews_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "features = pad_features(reviews_ints, seq_length = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Test and Validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_valid_x, train_y, test_valid_y = train_test_split(features, labels, test_size=0.2)\n",
    "\n",
    "valid_x, test_x, valid_y, test_y = train_test_split(test_valid_x,test_valid_y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # DEFINE LAYERS\n",
    "        \n",
    "        # embedding & LSTM\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout = drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # Output\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        #reshape\n",
    "        sig_out = sig_out.view(batch_size,-1)\n",
    "        sig_out = sig_out[:,-1]\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    def init_hidden(self,batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(74073, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = SentimentRNN(len(vocab_to_int)+1, 1, 400, 256, 2)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning Param\n",
    "lr=0.001\n",
    "# BinaryCrossEntropyLoss\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(74073, 400)\n",
       "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.3)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 50\n",
    "clip=5 \n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4... Step: 50... Loss: 0.673964... Val Loss: 0.666068\n",
      "Epoch: 1/4... Step: 100... Loss: 0.645453... Val Loss: 0.643612\n",
      "Epoch: 1/4... Step: 150... Loss: 0.616858... Val Loss: 0.629368\n",
      "Epoch: 1/4... Step: 200... Loss: 0.476110... Val Loss: 0.583556\n",
      "Epoch: 1/4... Step: 250... Loss: 0.646853... Val Loss: 0.592012\n",
      "Epoch: 1/4... Step: 300... Loss: 0.569554... Val Loss: 0.847540\n",
      "Epoch: 1/4... Step: 350... Loss: 0.726252... Val Loss: 0.695592\n",
      "Epoch: 1/4... Step: 400... Loss: 0.664741... Val Loss: 0.649504\n",
      "Epoch: 2/4... Step: 450... Loss: 0.503545... Val Loss: 0.594851\n",
      "Epoch: 2/4... Step: 500... Loss: 0.513118... Val Loss: 0.524070\n",
      "Epoch: 2/4... Step: 550... Loss: 0.468099... Val Loss: 0.516385\n",
      "Epoch: 2/4... Step: 600... Loss: 0.523744... Val Loss: 0.452503\n",
      "Epoch: 2/4... Step: 650... Loss: 0.495569... Val Loss: 0.481557\n",
      "Epoch: 2/4... Step: 700... Loss: 0.415736... Val Loss: 0.456158\n",
      "Epoch: 2/4... Step: 750... Loss: 0.430864... Val Loss: 0.405873\n",
      "Epoch: 2/4... Step: 800... Loss: 0.330600... Val Loss: 0.427546\n",
      "Epoch: 3/4... Step: 850... Loss: 0.386082... Val Loss: 0.386643\n",
      "Epoch: 3/4... Step: 900... Loss: 0.307240... Val Loss: 0.414856\n",
      "Epoch: 3/4... Step: 950... Loss: 0.131605... Val Loss: 0.392088\n",
      "Epoch: 3/4... Step: 1000... Loss: 0.309631... Val Loss: 0.399277\n",
      "Epoch: 3/4... Step: 1050... Loss: 0.263887... Val Loss: 0.382282\n",
      "Epoch: 3/4... Step: 1100... Loss: 0.255787... Val Loss: 0.466605\n",
      "Epoch: 3/4... Step: 1150... Loss: 0.317962... Val Loss: 0.372722\n",
      "Epoch: 3/4... Step: 1200... Loss: 0.362852... Val Loss: 0.445194\n",
      "Epoch: 4/4... Step: 1250... Loss: 0.335443... Val Loss: 0.379777\n",
      "Epoch: 4/4... Step: 1300... Loss: 0.362648... Val Loss: 0.420037\n",
      "Epoch: 4/4... Step: 1350... Loss: 0.160750... Val Loss: 0.421116\n",
      "Epoch: 4/4... Step: 1400... Loss: 0.184599... Val Loss: 0.406588\n",
      "Epoch: 4/4... Step: 1450... Loss: 0.159757... Val Loss: 0.402749\n",
      "Epoch: 4/4... Step: 1500... Loss: 0.101152... Val Loss: 0.407347\n",
      "Epoch: 4/4... Step: 1550... Loss: 0.324969... Val Loss: 0.485430\n",
      "Epoch: 4/4... Step: 1600... Loss: 0.257933... Val Loss: 0.390324\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    h = net.init_hidden(batch_size)\n",
    "    for inputs,labels in train_loader:\n",
    "        counter+=1\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "        \n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs,h)\n",
    "        \n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.432\n",
      "Test accuracy: 0.828\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "h = net.init_hidden(batch_size)\n",
    "net.eval()\n",
    "\n",
    "for inputs,labels in test_loader:\n",
    "    h = tuple([each.data for each in h ])\n",
    "    out,h = net(inputs,h)\n",
    "    \n",
    "    test_loss = criterion(out.squeeze(),labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    # convert out prob to int\n",
    "    pred = torch.round(out.squeeze())\n",
    "    \n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
